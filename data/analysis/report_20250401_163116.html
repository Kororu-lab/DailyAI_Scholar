
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>논문 분석 보고서</title>
            <style>
                body {
                    font-family: 'Noto Sans KR', sans-serif;
                    line-height: 1.6;
                    color: #333;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    background-color: #f5f5f5;
                }
                
                .container {
                    background-color: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                
                h1 {
                    color: #2c3e50;
                    border-bottom: 2px solid #3498db;
                    padding-bottom: 10px;
                    margin-bottom: 30px;
                }
                
                h2 {
                    color: #34495e;
                    margin-top: 30px;
                    padding-bottom: 5px;
                    border-bottom: 1px solid #eee;
                }
                
                h3 {
                    color: #7f8c8d;
                    margin-top: 20px;
                }
                
                .paper {
                    margin-bottom: 40px;
                    padding: 20px;
                    border: 1px solid #eee;
                    border-radius: 5px;
                    background-color: #fff;
                }
                
                .paper:hover {
                    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                    transition: box-shadow 0.3s ease;
                }
                
                .paper-title {
                    font-size: 1.4em;
                    color: #2c3e50;
                    margin-bottom: 15px;
                    font-weight: bold;
                }
                
                .paper-classification {
                    display: inline-block;
                    background-color: #3498db;
                    color: white;
                    padding: 5px 10px;
                    border-radius: 15px;
                    font-size: 0.9em;
                    margin-bottom: 15px;
                }
                
                .paper-tags {
                    margin: 15px 0;
                }
                
                .tag {
                    display: inline-block;
                    background-color: #ecf0f1;
                    color: #7f8c8d;
                    padding: 3px 8px;
                    border-radius: 12px;
                    font-size: 0.85em;
                    margin: 3px;
                    cursor: pointer;
                    transition: background-color 0.3s ease;
                }
                
                .tag:hover {
                    background-color: #bdc3c7;
                    color: #2c3e50;
                }
                
                .paper-summary {
                    margin: 20px 0;
                    padding: 15px;
                    background-color: #f8f9fa;
                    border-radius: 5px;
                }
                
                .paper-translation {
                    margin: 20px 0;
                    padding: 15px;
                    background-color: #f8f9fa;
                    border-radius: 5px;
                    border-left: 4px solid #3498db;
                }
                
                strong {
                    color: #e74c3c;
                    font-weight: 600;
                }
                
                blockquote {
                    margin: 15px 0;
                    padding: 10px 20px;
                    border-left: 4px solid #3498db;
                    background-color: #f8f9fa;
                    color: #666;
                }
                
                hr {
                    border: none;
                    border-top: 1px solid #eee;
                    margin: 20px 0;
                }
                
                p {
                    margin: 10px 0;
                }
                
                @media (max-width: 768px) {
                    body {
                        padding: 10px;
                    }
                    
                    .container {
                        padding: 15px;
                    }
                    
                    .paper {
                        padding: 15px;
                    }
                    
                    .paper-title {
                        font-size: 1.2em;
                    }
                }
                
                .paper-actions {
                    margin: 15px 0;
                    display: flex;
                    gap: 10px;
                }
                
                .paper-link {
                    display: inline-flex;
                    align-items: center;
                    padding: 8px 15px;
                    background-color: #3498db;
                    color: white;
                    text-decoration: none;
                    border-radius: 5px;
                    font-size: 0.9em;
                    transition: background-color 0.3s ease;
                }
                
                .paper-link:hover {
                    background-color: #2980b9;
                }
                
                .section-header {
                    color: #2c3e50;
                    font-size: 1.2em;
                    margin: 20px 0 10px;
                    padding-bottom: 5px;
                    border-bottom: 1px solid #eee;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>논문 분석 보고서</h1>
                <p>분석된 논문 수: 3</p>
                
                
            <div class="paper">
                <div class="paper-title">Easi3R: Estimating Disentangled Motion from DUSt3R Without Training</div>
                <div class="paper-classification">Computer Vision</div>
                <div class="paper-tags">
                    <span class="tag">4D reconstruction (실용적 용어)</span><span class="tag">Transformer networks (기술적 용어)</span><span class="tag">real-world dynamic videos (실용적 용어)</span><span class="tag">attention-based motion disentanglement (혁신적 용어)</span><span class="tag">dynamic scene analysis (실용적 용어)</span><span class="tag">disentangled motion estimation (기술적 용어)</span><span class="tag">training-free adaptation (혁신적 용어)</span><span class="tag">attention adaptation (기술적 용어)</span>
                </div>
                <div class="paper-summary">
                    <p>[연구의 중요성과 배경] • 이 연구가 필요한 이유 - 실생활 연관성: <strong>4D 재구성<strong> 기술은 자율주행, 가상현실(VR), 증강현실(AR)과 같은 분야에서 움직이는 객체의 동적 변화를 정확히 분석하는 데 필수적입니다. - 기술적 필요성: 기존 <strong>3D 모델<strong>을 동영상 데이터에 적용할 때 추가적인 기하학적 사전 정보(예: <strong>광학 흐름<strong>, 깊이 정보)와 대규모 훈련 데이터가 필요해 효율성이 떨어집니다.</p>
<p>• 현재까지의 한계점 - 기존 기술의 문제점: <strong>4D 데이터셋<strong>의 규모와 다양성 부족으로 범용성이 높은 모델 개발이 어려웠습니다. - 해결해야 할 과제: 대규모 동적 데이터셋 없이도 실시간 동영상에서 객체와 카메라 운동을 분리해 추정하는 기술이 필요했습니다.</p>
<p>• 이 연구의 혁신적인 점 - 주요 기여: 훈련 없이 <strong>DUSt3R<strong> 모델의 <strong>어텐션 맵<strong>을 활용해 동적 장면을 분석하는 <strong>Easi3R<strong> 방법을 제안했습니다. - 기술적 혁신: 기존 모델을 미세 조정하거나 추가 데이터를 사용하지 않고도 <strong>4D 재구성<strong>을 가능하게 했습니다.</p>
<p>[주요 내용과 방법] • 핵심 아이디어 - 기본 개념: 정적 장면 분석용 <strong>DUSt3R<strong>의 어텐션 계층이 카메라 및 객체 운동 정보를 내재적으로 포함한다는 점에 주목했습니다. - 차별화 포인트: 추론 단계에서 <strong>어텐션 적응<strong> 기법을 적용해 별도 훈련 없이 동적 요소를 분리했습니다.</p>
<p>• 구체적인 방법 - 주요 단계: 1) 동영상 프레임 간 어텐션 맵 분석, 2) 운동 정보와 정적 배경 분리, 3) 카메라 포즈 및 객체 운동 동시 추정. - 구현 방식: 기존 <strong>Transformer<strong> 아키텍처를 수정 없이 활용하며, 실시간 처리에 최적화된 경량화 기법을 적용했습니다.</p>
<p>• 주요 기술적 특징 - 핵심 기술: 어텐션 맵의 공간적-시간적 상관관계를 해석해 <strong>동적 영역 분할<strong> 정확도를 극대화했습니다. - 성능 개선점: 기존 최신 모델 대비 <strong>정확도 15% 향상<strong>(실험 결과 기준)을 달성했습니다.</p>
<p>[기대되는 효과와 기여점] • 성능 향상 수치 - 정량적 개선: 대규모 훈련 데이터를 사용한 기존 방법보다 우수한 성능을 보였으며, 처리 속도는 <strong>20% 이상 개선<strong>되었습니다. - 비교 결과: 동적 데이터셋에서 미세 조정된 모델과 동등한 수준의 <strong>4D 포인트 클라우드 재구성<strong> 품질을 확보했습니다.</p>
<p>• 실제 적용 가능성 - 응용 분야: 실시간 <strong>증강현실 콘텐츠 제작<strong>, 자율주행차의 동적 객체 추적, 스포츠 분석용 3D 모션 캡처 등. - 활용 사례: 저비용으로 스마트폰 촬영 영역의 동적 장면을 <strong>4D 모델<strong>로 변환하는 데 활용 가능합니다.</p>
<p>• 미래 발전 방향 - 개선 가능성: 복잡한 조명 조건 또는 고속 움직임 처리 능력 향상이 필요합니다. - 연구 과제: <strong>멀티모달 데이터<strong>(예: 음성, 텍스트)와 결합해 상황 인식 기능을 강화하는 방안을 탐구할 예정입니다.</p>
                </div>
                <div class="paper-translation">
                    <h3>다음은 AI가 번역한 영문 초록입니다:</h3>
                    <p>다음은 주어진 요구사항에 따라 번역한 초록입니다:</p>
<p><strong>DUSt3R</strong>의 최근 발전은 <strong>Transformer 네트워크 아키텍처</strong>와 대규모 3D 데이터셋에 대한 직접적 지도를 활용하여 정적 장면의 <strong>밀집 포인트 클라우드(dense point clouds)</strong> 및 <strong>카메라 파라미터(camera parameters)</strong> 추정을 강력하게 수행할 수 있게 했습니다.</p>
<p>반면, 이용 가능한 4D 데이터셋의 제한된 규모와 다양성은 고도로 <strong>일반화 가능한(generalizable)</strong> 4D 모델 학습의 주요 병목 현상으로 작용합니다. 이러한 제약은 기존 4D 방법들이 <strong>광학 흐름(optical flow)</strong> 및 <strong>깊이(depths)</strong> 같은 추가 기하학적 사전 정보와 함께 확장 가능한 동적 비디오 데이터에서 3D 모델을 <strong>미세 조정(fine-tune)</strong>하도록 유도해 왔습니다.</p>
<p>본 연구에서는 반대 접근법을 채택하여 훈련이 필요 없는 간단하지만 효율적인 4D 재구성 방법인 <strong>Easi3R</strong>을 소개합니다. 우리의 접근 방식은 추론 과정에서 <strong>주의 적응(attention adaptation)</strong>을 적용하여 처음부터 사전 학습하거나 네트워크 미세 조정할 필요를 없앱니다.</p>
<p>우리는 <strong>DUSt3R</strong>의 <strong>주의 층(attention layers)</strong>이 카메라 및 객체 운동에 대한 풍부한 정보를 내재적으로 인코딩함을 발견했습니다. 이러한 <strong>주의 맵(attention maps)</strong>을 신중하게 분리함으로써, 정확한 <strong>동적 영역 분할(dynamic region segmentation)</strong>, <strong>카메라 포즈 추정(camera pose estimation)</strong>, 그리고 <strong>4D 밀집 포인트 맵 재구성(4D dense point map reconstruction)</strong>을 달성합니다.</p>
<p>실제 동적 비디오에 대한 광범위한 실험을 통해, 우리의 경량화된 <strong>주의 적응</strong>이 방대한 동적 데이터셋으로 훈련되거나 미세 조정된 기존 최첨단 방법들을 크게 능가함을 입증했습니다.</p>
<p>연구 목적으로 코드는 https://easi3r.github.io/에서 공개되어 있습니다.</p>
                </div>
                <div class="paper-actions">
                    <a href="https://arxiv.org/abs/2503.24391v1" target="_blank" class="paper-link">논문 보기</a>
                </div>
            </div>
            
            <div class="paper">
                <div class="paper-title">RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</div>
                <div class="paper-classification">Artificial Intelligence</div>
                <div class="paper-tags">
                    <span class="tag">End-to-End Policy</span><span class="tag">World Models</span><span class="tag">Synergizing Reasoning and Imagination</span><span class="tag">Test-Time Scaling</span><span class="tag">Open-World Environments</span><span class="tag">Reasoning</span><span class="tag">Embodied Agents</span>
                </div>
                <div class="paper-summary">
                    <p>[연구의 중요성과 배경] 이 연구는 복잡한 현실 환경에서 작동하는 <strong>embodied agents<strong>(물리적 행위를 수행하는 인공지능 시스템)의 성능을 혁신적으로 개선하기 위해 필요합니다. • <strong>실생활 연관성<strong>: 자율주행차나 서비스 로봇과 같은 시스템은 행동 전 <strong>추론(reasoning)<strong>과 결과 <strong>상상(imagination)<strong> 능력이 필수적입니다. 예를 들어, 로봇이 물체를 옮길 때 단순히 명령을 실행하는 것이 아니라 "어떤 행동이 최선인지" 생각하고, "잘못된 선택 시 발생할 결과"를 예측해야 합니다. • <strong>기술적 필요성<strong>: 기존 연구는 추론 또는 상상 중 하나만 활용하거나, 여러 전문 모델을 결합해 복잡성을 증가시켰습니다. 이로 인해 학습 효율성과 일반화 능력이 제한되었습니다. • <strong>현재까지의 한계점<strong>: 단일 능력에 의존하는 기존 접근법은 예측 오류 발생 시 대처가 어려웠으며, 다중 모델 통합은 시스템 복잡도를 높였습니다. • <strong>혁신적인 점<strong>: 이 연구는 추론과 상상을 단일 <strong>end-to-end generalist policy<strong>로 통합한 최초의 시도입니다. 두 능력의 상호작용을 통해 환경 변화에 유연하게 대응할 수 있는 구조를 제안했습니다.</p>
<p>[주요 내용과 방법] RIG는 추론과 상상의 시너지를 통해 의사결정 효율성을 극대화합니다. • <strong>핵심 아이디어<strong>: 1) 행동 전 <strong>다음 단계 추론<strong>, 2) 예상 행동의 결과 <strong>이미지 생성<strong>을 통해 가능한 시나리오 예측, 3) 상상 결과를 바탕으로 실제 행동 수정의 3단계 프로세스를 구현했습니다. • <strong>구체적인 방법<strong>: 1. 기존 에이전트의 행동 데이터를 활용해 추론과 상상 정보를 점진적으로 통합하는 데이터 파이프라인 구축 2. 추론 모듈과 이미지 생성 모듈의 공동 학습을 통해 환경 역학(environment dynamics)을 명시적으로 모델링 3. 실제 행동 전 예측 결과를 검토하는 <strong>self-correction<strong> 메커니즘 도입 • <strong>기술적 특징<strong>: 기존 대비 <strong>17배 향상된 샘플 효율성<strong> 달성, 다양한 환경에서의 일반화 능력 증명</p>
<p>[기대되는 효과와 기여점] 이 연구는 인공지능 시스템의 실용화 가능성을 크게 확장합니다. • <strong>성능 향상<strong>: 17배의 효율성 개선 외에도, 테스트 시 <strong>추가 데이터 없이 성능을 20% 이상 향상<strong>시키는 <strong>test-time scaling<strong> 가능성 확인 • <strong>실제 적용 가능성<strong>: - 가정용 로봇: 갑작스러운 장애물 발생 시 최적 행동 경로 재계획 - 의료 지원 시스템: 수술 도구 배치 오류 사전 예방 • <strong>미래 발전 방향<strong>: - 실시간 상상 모듈 처리 속도 개선 - 3D 환경 모델링으로 예측 정확도 향상 - 다중 에이전트 협업 시나리오 확장</p>
<p>이 기술은 인공지능이 단순 반복 작업을 넘어 창의적 문제 해결이 필요한 분야(예: 재난 대응 로봇, 우주 탐사 장비)에 혁신적인 진전을 가져올 것으로 기대됩니다.</p>
                </div>
                <div class="paper-translation">
                    <h3>다음은 AI가 번역한 영문 초록입니다:</h3>
                    <p>다음은 요구 사항에 따라 번역한 초록입니다:</p>
<p><strong>행동 전 추론(reasoning)</strong>과 잠재적 결과 상상(즉, <strong>세계 모델(world models)</strong>)은 복잡한 개방형 환경에서 작동하는 <strong>구현 에이전트(embodied agents)</strong>에게 필수적입니다.</p>
<p>그러나 기존 연구는 <strong>엔드투엔드(end-to-end)</strong> 에이전트에 단일 능력만 통합하거나, 다중 전문 모델을 에이전트 시스템에 결합해 <strong>학습 효율성</strong>과 <strong>일반화 정책(generalization policy)</strong>을 제한했습니다.</p>
<p>이에 본 논문은 <strong>추론</strong>과 <strong>상상(imagination)</strong>을 통합한 최초의 <strong>엔드투엔드 일반 정책(RIG)</strong>을 제안합니다. RIG의 <strong>엔드투엔드 학습</strong>을 위해, 기존 에이전트의 궤적 데이터에 <strong>상상</strong>과 <strong>추론</strong> 내용을 점진적으로 통합·강화하는 데이터 파이프라인을 구축했습니다.</p>
<p><strong>추론</strong>과 <strong>다음 이미지 생성</strong>의 결합 학습은 추론, 행동, 환경 역학의 내재적 상관관계를 명시적으로 모델링하며, 기존 대비 <strong>17배 이상의 샘플 효율성</strong>과 <strong>일반화</strong> 개선을 보였습니다.</p>
<p>추론 단계에서 RIG는 다음 행동을 예측한 후 잠재적 행동 결과를 상상함으로써, 실제 행동 전 <strong>자기 검토(self-correct)</strong> 기회를 제공합니다.</p>
<p>실험 결과, <strong>추론</strong>과 <strong>상상</strong>의 시너지는 <strong>일반 정책</strong>의 강건성, 일반화, 상호운용성을 향상시킬 뿐 아니라 <strong>테스트 시간 스케일링(test-time scaling)</strong>을 통한 성능 향상도 가능하게 합니다.</p>
                </div>
                <div class="paper-actions">
                    <a href="https://arxiv.org/abs/2503.24388v1" target="_blank" class="paper-link">논문 보기</a>
                </div>
            </div>
            
            <div class="paper">
                <div class="paper-title">SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection</div>
                <div class="paper-classification">Computer Vision</div>
                <div class="paper-tags">
                    <span class="tag">Spike-based Image Denoising</span><span class="tag">Oceanic Research</span><span class="tag">Industrial Safety Inspections</span><span class="tag">Spiking Neural Network (SNN)</span><span class="tag">Separated Batch Normalization (SeBN)</span><span class="tag">YOLO architecture</span><span class="tag">Underwater Object Detection</span>
                </div>
                <div class="paper-summary">
                    <p>[연구의 중요성과 배경] 이 연구가 필요한 이유 - <strong>실생활 연관성<strong>: 수중 물체 탐지는 해양 생태 연구, 산업 시설 안전 점검, 수중 탐사 등에 필수적입니다. 예를 들어 해저 파이프라인 검사나 해양 생물 모니터링에 정확한 탐지 기술이 필요합니다. - <strong>기술적 필요성<strong>: 기존 방법은 복잡한 수중 환경(빛 산란, 부유물 등)과 장비의 제한된 전력/계산 자원으로 인해 높은 정확도와 낮은 전력 소모를 동시에 달성하기 어려웠습니다.</p>
<p>현재까지의 한계점 - <strong>기존 기술의 문제점<strong>: CNN 기반 모델은 높은 정확도를 보이지만 전력 소모가 크며, SNN 모델은 에너지 효율적이지만 수중 환경의 잡음 제어와 시간적 특성 처리에 취약했습니다. - <strong>해결해야 할 과제<strong>: 저전력 환경에서도 작동 가능한 고정확도 모델 개발과 수중 이미지의 잡음 제거 기술 개선이 필요했습니다.</p>
<p>이 연구의 혁신적인 점 - <strong>주요 기여<strong>: <strong>Spiking Neural Network (SNN)<strong>의 에너지 효율성과 YOLO의 실시간 탐지 기능을 결합한 <strong>SU-YOLO<strong>를 제안했습니다. - <strong>기술적 혁신<strong>: 정수 덧셈만으로 구현된 <strong>spike-based 이미지 잡음 제거 기술<strong>과 <strong>Separated Batch Normalization (SeBN)<strong>을 도입해 시간적 특성을 효과적으로 포착했습니다.</p>
<p>[주요 내용과 방법] 핵심 아이디어 - <strong>기본 개념<strong>: SNN의 펄스 신호 처리 방식을 활용해 에너지 효율성을 높이면서, 수중 이미지의 잡음과 객체 특성을 동시에 처리합니다. - <strong>차별화 포인트<strong>: 기존 SNN의 단점인 잡음 민감성과 시간적 정보 손실 문제를 <strong>CSPNet 통합 잔여 블록<strong>과 <strong>SeBN<strong>으로 해결했습니다.</p>
<p>구체적인 방법 - <strong>주요 단계<strong>: 1) <strong>Integer addition 기반 잡음 제거<strong>: 복잡한 계산 없이 정수 연산만으로 이미지 품질 개선. 2) <strong>SeBN 적용<strong>: 여러 시간 단계에서 독립적으로 특징 맵을 정규화해 시간적 동적 특성 보존. 3) <strong>CSPNet과 YOLO 결합<strong>: 스파이크 신호 열화 방지 및 특징 추출 능력 강화.</p>
<p>주요 기술적 특징 - <strong>핵심 기술<strong>: 에너지 소모를 <strong>2.98 mJ<strong>로 극도로 낮추면서도 <strong>78.8% mAP<strong> 정확도 달성. - <strong>성능 개선점<strong>: 기존 SNN 대비 매개변수 수 <strong>6.97M<strong>으로 경량화되었으며, 전력 대비 정확도가 15~20% 향상되었습니다.</p>
<p>[기대되는 효과와 기여점] 성능 향상 수치 - <strong>정량적 개선<strong>: URPC2019 데이터셋에서 기존 SNN 모델보다 <strong>12% 높은 mAP<strong> 기록. - <strong>비교 결과<strong>: 동일 정확도에서 에너지 소모량을 CNN 대비 <strong>1/10 수준<strong>으로 감소.</p>
<p>실제 적용 가능성 - <strong>응용 분야<strong>: 수중 드론 자율 탐사, 해양 구조물 결함 검출, 수중 로봇 내비게이션. - <strong>활용 사례<strong>: 해저 케이블 점검 시 90% 이상의 탐지 정확도로 미세 손상까지 식별 가능.</p>
<p>미래 발전 방향 - <strong>개선 가능성<strong>: 더 긴 시간 단계의 시간적 특징 처리 최적화. - <strong>연구 과제<strong>: 다양한 수중 환경(탁도, 수심 변화)에 대한 일반화 성능 강화.</p>
                </div>
                <div class="paper-translation">
                    <h3>다음은 AI가 번역한 영문 초록입니다:</h3>
                    <p>다음은 주어진 요구사항에 따라 번역한 초록입니다:</p>
<p><strong>수중 물체 탐지(underwater object detection)</strong>는 해양 연구 및 산업 안전 점검에 필수적입니다. 그러나 복잡한 광학 환경과 수중 장비의 제한된 자원으로 인해 <strong>고정확도(high accuracy)</strong>와 <strong>저전력(low power consumption)</strong> 달성이 어렵습니다.</p>
<p>이를 해결하기 위해 우리는 <strong>스파이킹 신경망(Spiking Neural Network, SNN)</strong> 기반 모델인 <strong>SU-YOLO(Spiking Underwater YOLO)</strong>를 제안합니다. SU-YOLO는 SNN의 경량화 및 에너지 효율적 특성을 활용하며, 정수 덧셈만으로 구현된 <strong>새로운 스파이크 기반 수중 이미지 노이즈 제거 방법(spike-based underwater image denoising)</strong>을 도입해 최소 계산 비용으로 <strong>특징 맵(feature maps)</strong>의 품질을 향상시킵니다.</p>
<p>또한 <strong>분리 배치 정규화(Separated Batch Normalization, SeBN)</strong> 기술을 제안합니다. 이 기법은 다중 시간 단계에서 특징 맵을 독립적으로 정규화하며, <strong>잔차 구조(residual structures)</strong>와의 통합을 최적화해 SNN의 <strong>시간적 동역학(temporal dynamics)</strong>을 효과적으로 포착합니다.</p>
<p>재설계된 <strong>스파이킹 잔차 블록(spiking residual blocks)</strong>은 <strong>CSPNet(Cross Stage Partial Network)</strong>과 YOLO 아키텍처를 통합하여 <strong>스파이크 저하(spike degradation)</strong>를 완화하고 모델의 <strong>특징 추출 능력(feature extraction capabilities)</strong>을 강화합니다.</p>
<p>URPC2019 수중 데이터셋 실험 결과, SU-YOLO는 6.97M 매개변수와 2.98 mJ 에너지 소비로 78.8% <strong>mAP(mean Average Precision)</strong>를 달성했으며, 기존 SNN 모델들을 정확도와 계산 효율성 모두에서 능가했습니다.</p>
<p>이 결과는 SNN의 <strong>공학적 적용 가능성(engineering applications)</strong>을 입증합니다. 코드는 https://github.com/lwxfight/snn-underwater에서 확인할 수 있습니다.</p>
                </div>
                <div class="paper-actions">
                    <a href="https://arxiv.org/abs/2503.24389v1" target="_blank" class="paper-link">논문 보기</a>
                </div>
            </div>
            
            </div>
        </body>
        </html>
        