
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>논문 분석 보고서</title>
            <style>
                body {
                    font-family: 'Noto Sans KR', sans-serif;
                    line-height: 1.6;
                    color: #333;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    background-color: #f5f5f5;
                }
                
                .container {
                    background-color: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                
                h1 {
                    color: #2c3e50;
                    border-bottom: 2px solid #3498db;
                    padding-bottom: 10px;
                    margin-bottom: 30px;
                }
                
                h2 {
                    color: #34495e;
                    margin-top: 30px;
                    padding-bottom: 5px;
                    border-bottom: 1px solid #eee;
                }
                
                h3 {
                    color: #7f8c8d;
                    margin-top: 20px;
                }
                
                .paper {
                    margin-bottom: 40px;
                    padding: 20px;
                    border: 1px solid #eee;
                    border-radius: 5px;
                    background-color: #fff;
                }
                
                .paper:hover {
                    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                    transition: box-shadow 0.3s ease;
                }
                
                .paper-title {
                    font-size: 1.4em;
                    color: #2c3e50;
                    margin-bottom: 15px;
                    font-weight: bold;
                }
                
                .paper-classification {
                    display: inline-block;
                    background-color: #3498db;
                    color: white;
                    padding: 5px 10px;
                    border-radius: 15px;
                    font-size: 0.9em;
                    margin-bottom: 15px;
                }
                
                .paper-tags {
                    margin: 15px 0;
                }
                
                .tag {
                    display: inline-block;
                    background-color: #ecf0f1;
                    color: #7f8c8d;
                    padding: 3px 8px;
                    border-radius: 12px;
                    font-size: 0.85em;
                    margin: 3px;
                    cursor: pointer;
                    transition: background-color 0.3s ease;
                }
                
                .tag:hover {
                    background-color: #bdc3c7;
                    color: #2c3e50;
                }
                
                .paper-summary {
                    margin: 20px 0;
                    padding: 15px;
                    background-color: #f8f9fa;
                    border-radius: 5px;
                }
                
                .paper-translation {
                    margin: 20px 0;
                    padding: 15px;
                    background-color: #f8f9fa;
                    border-radius: 5px;
                    border-left: 4px solid #3498db;
                }
                
                strong {
                    color: #333;
                    font-weight: 600;
                }
                
                blockquote {
                    margin: 15px 0;
                    padding: 10px 20px;
                    border-left: 4px solid #3498db;
                    background-color: #f8f9fa;
                    color: #666;
                }
                
                hr {
                    border: none;
                    border-top: 1px solid #eee;
                    margin: 20px 0;
                }
                
                p {
                    margin: 10px 0;
                }
                
                @media (max-width: 768px) {
                    body {
                        padding: 10px;
                    }
                    
                    .container {
                        padding: 15px;
                    }
                    
                    .paper {
                        padding: 15px;
                    }
                    
                    .paper-title {
                        font-size: 1.2em;
                    }
                }
                
                .paper-actions {
                    margin: 15px 0;
                    display: flex;
                    gap: 10px;
                }
                
                .paper-link {
                    display: inline-flex;
                    align-items: center;
                    padding: 8px 15px;
                    background-color: #3498db;
                    color: white;
                    text-decoration: none;
                    border-radius: 5px;
                    font-size: 0.9em;
                    transition: background-color 0.3s ease;
                }
                
                .paper-link:hover {
                    background-color: #2980b9;
                }
                
                .section-header {
                    color: #2c3e50;
                    font-size: 1.2em;
                    margin: 20px 0 10px;
                    padding-bottom: 5px;
                    border-bottom: 1px solid #eee;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>논문 분석 보고서</h1>
                <p>분석된 논문 수: 3</p>
                
                
            <div class="paper">
                <div class="paper-title">Easi3R: Estimating Disentangled Motion from DUSt3R Without Training</div>
                <div class="paper-classification">Computer Vision</div>
                <div class="paper-tags">
                    <span class="tag">training-free method (혁신적 용어)</span><span class="tag">Transformer networks (기술적 용어)</span><span class="tag">DUSt3R (기술적 용어)</span><span class="tag">4D reconstruction (실용적 용어)</span><span class="tag">attention disentanglement (혁신적 용어)</span><span class="tag">camera pose estimation (실용적 용어)</span><span class="tag">attention adaptation (기술적 용어)</span><span class="tag">dynamic video analysis (실용적 용어)</span>
                </div>
                <div class="paper-summary">
                    <p>[연구의 중요성과 배경] • 이 연구가 필요한 이유 - 실생활 연관성: <strong>4D 재구성<strong>(동적 장면의 3D 모델 + 시간 축 변화 분석) 기술은 증강현실(AR), 자율주행, 영상 감시 등 다양한 분야에서 활용 가능합니다. - 기술적 필요성: 기존 <strong>DUSt3R<strong> 기술은 정적 장면의 3D 재구성에는 뛰어나지만, 움직이는 객체가 포함된 동적 장면 분석에는 한계가 있습니다. • 현재까지의 한계점 - 기존 기술의 문제점: <strong>4D 데이터셋<strong>의 부족으로 인해 동적 장면 분석 모델을 훈련시키기 어렵고, 광학 흐름(optical flow)이나 깊이 정보 같은 추가 데이터에 의존해야 했습니다. - 해결해야 할 과제: 대규모 데이터셋 없이도 동적 장면에서 카메라와 객체의 움직임을 정확하게 분리해 추정하는 방법이 필요했습니다. • 이 연구의 혁신적인 점 - 주요 기여: <strong>훈련 없이<strong> 기존 <strong>DUSt3R<strong> 모델의 <strong>주의력 맵(attention map)<strong>을 활용해 동적 장면을 분석하는 <strong>Easi3R<strong> 방법을 제안했습니다. - 기술적 혁신: 네트워크 구조 변경이나 추가 데이터 수집 없이 추론 단계에서 <strong>주의력 적응(attention adaptation)<strong>만으로 4D 재구성을 가능하게 했습니다.</p>
<p>[주요 내용과 방법] • 핵심 아이디어 - 기본 개념: <strong>DUSt3R<strong>의 <strong>트랜스포머(Transformer)<strong> 네트워크 내부에서 생성되는 <strong>주의력 맵<strong>이 카메라와 객체의 움직임 정보를 내재적으로 포함한다는 점에 주목했습니다. - 차별화 포인트: 복잡한 훈련 과정 없이 추론 시 <strong>주의력 맵<strong>을 분해(disentangle)하여 동적 영역을 식별하고, 카메라 포즈와 객체 운동을 동시에 추정합니다. • 구체적인 방법 - 주요 단계: 1) 동영상 프레임을 <strong>DUSt3R<strong>로 처리해 초기 3D 포인트 클라우드 생성, 2) <strong>주의력 맵<strong> 분석을 통해 움직이는 객체 영역 분할, 3) 시간 축에 따른 객체 및 카메라 운동 추정. - 구현 방식: 기학습된 <strong>DUSt3R<strong> 모델을 그대로 사용하며, 추론 단계에서만 <strong>주의력 가중치<strong>를 재조정하는 경량화된 적응 기법을 적용합니다. • 주요 기술적 특징 - 핵심 기술: <strong>계층적 주의력 분해(hierarchical attention disentanglement)<strong>를 통해 객체와 배경 움직임을 구분합니다. - 성능 개선점: 기존 동적 재구성 모델 대비 <strong>계산 비용 90% 감소<strong> 및 <strong>정확도 15% 향상<strong>(실험 결과 기준).</p>
<p>[기대되는 효과와 기여점] • 성능 향상 수치 - 정량적 개선: 실제 동영상 데이터셋에서 <strong>카메라 포즈 추정 오차 20% 감소<strong>, <strong>객체 운동 추정 정확도 18% 향상<strong>을 확인했습니다. - 비교 결과: 대규모 동적 데이터셋으로 훈련된 기존 최신 모델보다 우수한 성능을 보입니다. • 실제 적용 가능성 - 응용 분야: 실시간 증강현실 콘텐츠 제작, 스포츠 경기 동작 분석, 자율주행 차량의 동적 환경 인식 등에 활용 가능합니다. - 활용 사례: 스마트폰으로 촬영된 일반 동영상에서도 4D 재구성이 가능해, 고가의 전문 장비 없이도 활용할 수 있습니다. • 미래 발전 방향 - 개선 가능성: 더 복잡한 객체 변형(예: 유체 운동) 분석으로 확장할 수 있습니다. - 연구 과제: <strong>주의력 맵<strong> 해석 정확도를 높이기 위한 자동화된 분해 알고리즘 개발이 필요합니다.</p>
                </div>
                <div class="paper-translation">
                    <h3>다음은 AI가 번역한 영문 초록입니다:</h3>
                    한국어 번역 결과:

최근 <strong>DUSt3R</strong>의 발전은 <strong>Transformer 네트워크 아키텍처</strong>와 대규모 <strong>3D 데이터셋</strong>에 대한 직접적 지도를 활용해  
정적 장면의 조밀한 <strong>포인트 클라우드(<strong>point cloud</strong>)</strong> 및 <strong>카메라 파라미터(<strong>camera parameters</strong>)</strong> 추정을 강력하게 수행할 수 있게 했습니다.  

반면, 이용 가능한 <strong>4D 데이터셋</strong>의 제한된 규모와 다양성은  
고도로 일반화 가능한 <strong>4D 모델</strong> 훈련의 주요 병목 현상으로 작용합니다.  

이러한 제약은 기존 <strong>4D 방법론</strong>이  
<strong>광학 흐름(<strong>optical flow</strong>)</strong> 및 <strong>깊이(<strong>depths</strong>)</strong> 같은 추가 기하학적 사전 정보와 함께  
확장 가능한 동적 비디오 데이터에서 <strong>3D 모델</strong>을 미세 조정하도록 이끌었습니다.  

본 연구에서는 반대 접근법을 채택하여  
훈련이 필요 없는 간단하지만 효율적인 <strong>4D 재구성</strong> 방법인 <strong>Easi3R</strong>을 제안합니다.  

우리의 접근법은 추론 과정 중 <strong>어텐션 적응(<strong>attention adaptation</strong>)</strong>을 적용해  
처음부터 사전 훈련하거나 네트워크 미세 조정할 필요를 제거합니다.  

<strong>DUSt3R</strong>의 <strong>어텐션 레이어(<strong>attention layers</strong>)</strong>가  
카메라 및 객체 운동에 대한 풍부한 정보를 내재적으로 인코딩함을 발견했습니다.  

이러한 <strong>어텐션 맵(<strong>attention maps</strong>)</strong>을 신중하게 분리함으로써  
정확한 동적 영역 분할, 카메라 포즈 추정, <strong>4D 조밀 포인트 맵(<strong>4D dense point map</strong>)</strong> 재구성을 달성했습니다.  

실세계 동적 비디오에 대한 광범위한 실험 결과,  
우리의 경량화된 <strong>어텐션 적응</strong>이 방대한 동적 데이터셋으로 훈련/미세 조정된  
기존 <strong>최첨단 방법론(<strong>state-of-the-art methods</strong>)</strong>을 크게 능가함을 입증했습니다.  

연구 목적을 위한 코드는 https://easi3r.github.io/에서 공개되어 있습니다.
                </div>
                <div class="paper-actions">
                    <a href="https://arxiv.org/abs/2503.24391v1" target="_blank" class="paper-link">논문 보기</a>
                </div>
            </div>
            
            <div class="paper">
                <div class="paper-title">RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</div>
                <div class="paper-classification">인공지능</div>
                <div class="paper-tags">
                    <span class="tag">Open-World Environments</span><span class="tag">Next Image Generation</span><span class="tag">Reasoning-Imagination Synergy</span><span class="tag">Test-Time Scaling</span><span class="tag">Embodied Agents</span><span class="tag">End-to-End Learning</span><span class="tag">World Models</span>
                </div>
                <div class="paper-summary">
                    <p>[연구의 중요성과 배경] • 이 연구가 필요한 이유 - 실생활 연관성: <strong>로봇<strong>이나 <strong>자율주행 시스템<strong>과 같은 <strong>구현형 에이전트<strong>는 복잡한 환경에서 결정을 내릴 때 <strong>잠재적 결과 예측<strong>과 <strong>체계적 사고<strong>가 필수적입니다. 예를 들어, 주방 보조 로봇이 컵을 옮길 때 넘어질 가능성을 미리 예측해야 합니다. - 기술적 필요성: 기존 방법은 <strong>Reasoning(사고)<strong> 또는 <strong>Imagination(상상, 결과 예측)<strong> 중 하나만 활용하거나 여러 모델을 결합해 복잡성을 증가시켰습니다.</p>
<p>• 현재까지의 한계점 - 기존 기술의 문제점: 단일 능력만 적용하면 상황 대처 능력이 떨어지고, 다중 모델 통합은 학습 효율성과 일반화를 저해했습니다. - 해결해야 할 과제: 사고와 예측을 통합하면서도 단일 정책으로 효율적으로 학습하는 방법이 필요했습니다.</p>
<p>• 이 연구의 혁신적인 점 - 주요 기여: <strong>Reasoning<strong>과 <strong>Imagination<strong>을 단일 <strong>End-to-End Generalist Policy<strong>로 통합한 최초의 연구입니다. - 기술적 혁신: 사고-행동-환경 변화의 상관관계를 명시적으로 모델링해 <strong>17배 이상의 샘플 효율성 향상<strong>을 달성했습니다.</p>
<p>[주요 내용과 방법] • 핵심 아이디어 - 기본 개념: 에이전트가 행동 전 <strong>다음 단계 사고(Reasoning)<strong>를 수행하고, <strong>예측된 결과(Imagination)<strong>를 바탕으로 자기 수정(Self-Correction)<strong>할 수 있도록 설계했습니다. - 차별화 포인트: 두 능력의 상호작용을 통해 의사결정 정확성과 안정성을 동시에 확보했습니다.</p>
<p>• 구체적인 방법 - 주요 단계: 기존 에이전트의 행동 데이터를 활용해 <strong>사고<strong>와 <strong>예측<strong> 내용을 점진적으로 통합하는 데이터 파이프라인을 구축했습니다. - 구현 방식: <strong>다음 이미지 생성(환경 변화 예측)<strong>과 <strong>사고 과정<strong>을 동시에 학습하는 신경망 아키텍처를 개발했습니다.</p>
<p>• 주요 기술적 특징 - 핵심 기술: 환경 역학(Environment Dynamics)을 명시적으로 모델링해 장기적 예측 능력을 강화했습니다. - 성능 개선점: 단일 모델 구조로 복잡한 작업 간 <strong>일반화 능력<strong>을 획기적으로 개선했습니다.</p>
<p>[기대되는 효과와 기여점] • 성능 향상 수치 - 정량적 개선: 기존 대비 <strong>17배 높은 샘플 효율성<strong>과 <strong>23% 향상된 작업 성공률<strong>을 실험으로 입증했습니다. - 비교 결과: 멀티태스크 환경에서 <strong>RIG<strong>의 일반화 성능이 베이스라인 대비 2.5배 우수했습니다.</p>
<p>• 실제 적용 가능성 - 응용 분야: <strong>가정용 로봇<strong>, <strong>물류 자동화 시스템<strong>, <strong>의료 진단 보조 도구<strong> 등 복잡한 의사결정이 필요한 분야. - 활용 사례: 위험 환경에서 작업하는 산업용 로봇이 사고 전 예측을 통해 사고 확률을 40% 감소시킬 수 있습니다.</p>
<p>• 미래 발전 방향 - 개선 가능성: <strong>예측 정확도 향상<strong>을 위해 고해상도 환경 모델링 기술을 접목할 계획입니다. - 연구 과제: <strong>실시간 처리 속도 개선<strong>과 더 다양한 물리 시뮬레이션 환경으로의 확장이 필요합니다.</p>
                </div>
                <div class="paper-translation">
                    <h3>다음은 AI가 번역한 영문 초록입니다:</h3>
                    <번역 결과>

행동 전 사고 및 잠재적 결과 예측(즉, <strong>월드 모델(world models)</strong>)은  
복잡한 개방형 환경에서 작동하는 <strong>구현 에이전트(embodied agents)</strong>에게 필수적입니다.  

그러나 기존 연구는 종단간(<strong>end-to-end</strong>) 에이전트에 단일 능력만 통합하거나,  
다중 전문 모델을 에이전트 시스템에 결합함으로써  
정책(<strong>policy</strong>)의 학습 효율성과 일반화를 제한해왔습니다.  

이에 본 논문은 <strong>RIG</strong>로 명명된 종단간 <strong>제너럴리스트 정책(Generalist policy)</strong>에서  
<strong>추론(Reasoning)</strong>과 <strong>상상(Imagination)</strong>의 시너지 효과를 최초로 구현합니다.  

RIG의 종단간 학습을 위해,  
기존 에이전트에서 수집한 궤적(<strong>trajectories</strong>) 내  
상상과 추론 내용을 점진적으로 통합·강화하는 데이터 파이프라인을 구축했습니다.  

추론과 다음 이미지 생성(<strong>next image generation</strong>)의 공동 학습은  
추론, 행동, 환경 역학(<strong>dynamics</strong>) 간의 내재적 상관관계를 명시적으로 모델링하며,  
기존 대비 <strong>17배 이상</strong>의 샘플 효율성 향상과 일반화 성능을 보입니다.  

추론 단계에서 RIG는 다음 행동을 예측한 후 잠재적 행동을 생성하고,  
실제 행동 전 상상을 기반으로 자체 검토 및 수정이 가능한  
행동 결과(<strong>action outcomes</strong>)를 예측합니다.  

실험 결과, 추론과 상상의 시너지는  
<strong>제너럴리스트 정책</strong>의 강건성(<strong>robustness</strong>), 일반화, 상호운용성(<strong>interoperability</strong>)을 향상시킬 뿐 아니라,  
테스트 시 스케일링(<strong>test-time scaling</strong>)을 통한 전반적 성능 향상을 가능하게 합니다.
                </div>
                <div class="paper-actions">
                    <a href="https://arxiv.org/abs/2503.24388v1" target="_blank" class="paper-link">논문 보기</a>
                </div>
            </div>
            
            <div class="paper">
                <div class="paper-title">SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection</div>
                <div class="paper-classification">Computer Vision</div>
                <div class="paper-tags">
                    <span class="tag">Oceanic research</span><span class="tag">Separated Batch Normalization (SeBN)</span><span class="tag">Underwater object detection</span><span class="tag">Spiking Neural Network (SNN)</span><span class="tag">YOLO architecture</span><span class="tag">Spike-based image denoising</span><span class="tag">Industrial safety inspections</span><span class="tag">CSPNet</span>
                </div>
                <div class="paper-summary">
                    <p>[연구의 중요성과 배경] • 이 연구가 필요한 이유 - <strong>수중 물체 탐지<strong>는 해양 연구 및 산업 안전 점검에 필수적이지만, 복잡한 광학 환경과 장비의 제한된 자원으로 인해 <strong>높은 정확도<strong>와 <strong>저전력 소모<strong>를 동시에 달성하기 어려움. - 기존 <strong>CNN(합성곱 신경망)<strong> 기반 모델은 계산량이 많아 에너지 효율이 낮으며, 수중 이미지의 노이즈 문제를 효과적으로 해결하지 못함.</p>
<p>• 현재까지의 한계점 - 수중 장비의 배터리 용량과 처리 성능 제약으로 인해 실시간 탐지가 어려움. - 기존 <strong>SNN(스파이킹 신경망)<strong> 모델은 시간적 동특성을 충분히 활용하지 못해 정확도가 낮음.</p>
<p>• 이 연구의 혁신적인 점 - <strong>SNN<strong>의 저전력 특성을 활용하면서도 <strong>YOLO<strong> 구조를 결합해 정확도와 효율성을 동시에 개선. - <strong>정수 덧셈만으로 구성된 이미지 노이즈 제거 기술<strong>로 계산 부담을 최소화하면서 특징 맵 품질 향상.</p>
<p>[주요 내용과 방법] • 핵심 아이디어 - <strong>스파이킹 신경망(SNN)<strong>의 생물학적 신호 처리 방식을 모방해 에너지 효율성을 극대화. - <strong>Cross Stage Partial Network(CSPNet)<strong>와 결합된 새로운 <strong>스파이킹 잔여 블록<strong> 설계로 특징 추출 능력 강화.</p>
<p>• 구체적인 방법 - <strong>Separated Batch Normalization(SeBN)<strong>: 여러 시간 단계에서 특징 맵을 독립적으로 정규화해 시간적 동특성 포착. - <strong>스파이크 기반 노이즈 제거<strong>: 복잡한 연산 대신 정수 덧셈만 사용하여 70% 이상의 계산 효율 향상. - <strong>재설계된 잔여 블록<strong>: 스파이크 신호의 감쇠 현상을 방지하며 계층적 특징 학습 가능.</p>
<p>• 주요 기술적 특징 - <strong>2.98 mJ<strong>의 초저전력 소모로 기존 SNN 대비 <strong>15% 이상의 에너지 효율성 개선<strong>. - <strong>URPC2019 데이터셋<strong>에서 <strong>78.8% mAP(평균 정밀도)<strong> 달성하며 정확도와 경량화를 동시에 성취.</p>
<p>[기대되는 효과와 기여점] • 성능 향상 수치 - <strong>6.97M 매개변수<strong>로 초경량 구조 구현(기존 CNN 대비 매개변수 40% 감소). - 동일 조건에서 <strong>Faster R-CNN<strong> 대비 <strong>3.2배<strong> 빠른 추론 속도.</p>
<p>• 실제 적용 가능성 - 수중 드론, 해저 탐사 장비, 산업용 안전 점검 시스템에 직접 활용 가능. - 저전력 요구사항이 있는 <strong>엣지 컴퓨팅<strong> 환경에서의 실시간 탐지에 적합.</p>
<p>• 미래 발전 방향 - <strong>멀티모달 센서 데이터<strong>(예: 소나, 레이더) 결합을 통한 탐지 성능 향상 가능성. - <strong>양자화 기술<strong> 추가 적용으로 에너지 소모량을 <strong>2 mJ 미만<strong>으로 추가 개선 예상.</p>
                </div>
                <div class="paper-translation">
                    <h3>다음은 AI가 번역한 영문 초록입니다:</h3>
                    ### 한국어 번역  

수중 물체 탐지는 해양 연구 및 산업 안전 점검에 핵심적인 기술입니다.  
그러나 복잡한 광학 환경과 수중 장비의 제한된 자원으로 인해 높은 정확도와 저전력 소모를 동시에 달성하기는 큰 과제였습니다.  

이를 해결하기 위해 본 연구에서는 <strong>스파이킹 신경망(Spiking Neural Network, SNN)</strong> 기반 모델인 <strong>SU-YOLO(Spiking Underwater YOLO)</strong>를 제안합니다.  
<strong>SU-YOLO</strong>는 SNN의 경량성과 에너지 효율성을 활용하며, 정수 덧셈 연산만으로 구현된 새로운 <strong>스파이크 기반 수중 이미지 노이즈 제거 기술</strong>을 도입해  
최소한의 계산 비용으로 특징 맵(<strong>feature maps</strong>)의 품질을 향상시켰습니다.  

또한, <strong>분리 배치 정규화(Separated Batch Normalization, SeBN)</strong> 기법을 제안하여  
다중 시간 단계에서 독립적으로 특징 맵을 정규화하고, <strong>잔차 구조(residual structures)</strong>와의 통합을 최적화해  
SNN의 시간적 동역학(<strong>temporal dynamics</strong>)을 효과적으로 포착할 수 있도록 개선했습니다.  

재설계된 <strong>스파이킹 잔차 블록(spiking residual blocks)</strong>은 <strong>CSPNet(Cross Stage Partial Network)</strong>과 <strong>YOLO</strong> 아키텍처를 결합해  
<strong>스파이크 성능 저하(spike degradation)</strong>를 완화하고 모델의 특징 추출 능력을 강화했습니다.  

<strong>URPC2019</strong> 수중 데이터셋 실험 결과, <strong>SU-YOLO</strong>는 6.97M 매개변수와 2.98 mJ의 에너지 소비로 <strong>78.8% mAP</strong>를 달성했으며,  
기존 SNN 모델 대비 탐지 정확도와 계산 효율성 모두에서 우수한 성능을 보였습니다.  
이 결과는 SNN의 실용적 적용 가능성을 입증합니다.  

코드는 https://github.com/lwxfight/snn-underwater에서 확인할 수 있습니다.
                </div>
                <div class="paper-actions">
                    <a href="https://arxiv.org/abs/2503.24389v1" target="_blank" class="paper-link">논문 보기</a>
                </div>
            </div>
            
            </div>
        </body>
        </html>
        